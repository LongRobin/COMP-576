{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-1-a098afbff342>, line 207)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-a098afbff342>\"\u001b[1;36m, line \u001b[1;32m207\u001b[0m\n\u001b[1;33m    if __name__ == \"__main__\":\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "__author__ = 'tan_nguyen'\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_data():\n",
    "    '''\n",
    "    generate data\n",
    "    :return: X: input data, y: given labels\n",
    "    '''\n",
    "    np.random.seed(0)\n",
    "    X, y = datasets.make_moons(200, noise=0.20)\n",
    "    return X, y\n",
    "\n",
    "def plot_decision_boundary(pred_func, X, y):\n",
    "    '''\n",
    "    plot the decision boundary\n",
    "    :param pred_func: function used to predict the label\n",
    "    :param X: input data\n",
    "    :param y: given labels\n",
    "    :return:\n",
    "    '''\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole gid\n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)\n",
    "    plt.show()\n",
    "\n",
    "########################################################################################################################\n",
    "########################################################################################################################\n",
    "# YOUR ASSSIGMENT STARTS HERE\n",
    "# FOLLOW THE INSTRUCTION BELOW TO BUILD AND TRAIN A 3-LAYER NEURAL NETWORK\n",
    "########################################################################################################################\n",
    "########################################################################################################################\n",
    "class NeuralNetwork(object):\n",
    "    \"\"\"\n",
    "    This class builds and trains a neural network\n",
    "    \"\"\"\n",
    "    def __init__(self, nn_input_dim, nn_hidden_dim , nn_output_dim, actFun_type='tanh', reg_lambda=0.01, seed=0):\n",
    "        '''\n",
    "        :param nn_input_dim: input dimension\n",
    "        :param nn_hidden_dim: the number of hidden units\n",
    "        :param nn_output_dim: output dimension\n",
    "        :param actFun_type: type of activation function. 3 options: 'tanh', 'sigmoid', 'relu'\n",
    "        :param reg_lambda: regularization coefficient\n",
    "        :param seed: random seed\n",
    "        '''\n",
    "        self.nn_input_dim = nn_input_dim\n",
    "        self.nn_hidden_dim = nn_hidden_dim\n",
    "        self.nn_output_dim = nn_output_dim\n",
    "        self.actFun_type = actFun_type\n",
    "        self.reg_lambda = reg_lambda\n",
    "        \n",
    "        # initialize the weights and biases in the network\n",
    "        np.random.seed(seed)\n",
    "        self.W1 = np.random.randn(self.nn_input_dim, self.nn_hidden_dim) / np.sqrt(self.nn_input_dim)\n",
    "        self.b1 = np.zeros((1, self.nn_hidden_dim))\n",
    "        self.W2 = np.random.randn(self.nn_hidden_dim, self.nn_output_dim) / np.sqrt(self.nn_hidden_dim)\n",
    "        self.b2 = np.zeros((1, self.nn_output_dim))\n",
    "\n",
    "    def actFun(self, z, type):\n",
    "        '''\n",
    "        actFun computes the activation functions\n",
    "        :param z: net input\n",
    "        :param type: Tanh, Sigmoid, or ReLU\n",
    "        :return: activations\n",
    "        '''\n",
    "\n",
    "        # YOU IMPLMENT YOUR actFun HERE\n",
    "        if type=='tanh':\n",
    "            actFunction = np.tanh(z) \n",
    "        if type=='sigmoid':\n",
    "            actFunction = 1 / (1 + np.exp(-x))\n",
    "        if type=='relu':\n",
    "            actFunction = np.maximum(0,z)\n",
    "\n",
    "        return actFunction\n",
    "\n",
    "    def diff_actFun(self, z, type):\n",
    "        '''\n",
    "        diff_actFun computes the derivatives of the activation functions wrt the net input\n",
    "        :param z: net input\n",
    "        :param type: Tanh, Sigmoid, or ReLU\n",
    "        :return: the derivatives of the activation functions wrt the net input\n",
    "        '''\n",
    "\n",
    "        # YOU IMPLEMENT YOUR diff_actFun HERE\n",
    "        if type=='tanh':\n",
    "            diff_act = 0.5*np.log(1+z) - 0.5*np.log(1-z)\n",
    "        if type=='sigmoid':\n",
    "            diff_act = np.exp(-x) / np.power((1 + np.exp(-x)),2)\n",
    "        if type=='relu':\n",
    "            diff_act = np.zeros(z.shape[0]) \n",
    "            diff_act[z>0] = 1\n",
    "        \n",
    "\n",
    "        return diff_act\n",
    "\n",
    "    def feedforward(self, X, actFun):\n",
    "        '''\n",
    "        feedforward builds a 3-layer neural network and computes the two probabilities,\n",
    "        one for class 0 and one for class 1\n",
    "        :param X: input data\n",
    "        :param actFun: activation function\n",
    "        :return:\n",
    "        '''\n",
    "\n",
    "        # YOU IMPLEMENT YOUR feedforward HERE\n",
    "\n",
    "         self.z1 = actFun(X)\n",
    "         self.a1 = \n",
    "         self.z2 =\n",
    "        exp_scores = np.exp(self.z2)\n",
    "        self.probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        return None\n",
    "\n",
    "    def calculate_loss(self, X, y):\n",
    "        '''\n",
    "        calculate_loss computes the loss for prediction\n",
    "        :param X: input data\n",
    "        :param y: given labels\n",
    "        :return: the loss for prediction\n",
    "        '''\n",
    "        num_examples = len(X)\n",
    "        self.feedforward(X, lambda x: self.actFun(x, type=self.actFun_type))\n",
    "        # Calculating the loss\n",
    "\n",
    "        # YOU IMPLEMENT YOUR CALCULATION OF THE LOSS HERE\n",
    "\n",
    "        # data_loss =\n",
    "\n",
    "        # Add regulatization term to loss (optional)\n",
    "        data_loss += self.reg_lambda / 2 * (np.sum(np.square(self.W1)) + np.sum(np.square(self.W2)))\n",
    "        return (1. / num_examples) * data_loss\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        predict infers the label of a given data point X\n",
    "        :param X: input data\n",
    "        :return: label inferred\n",
    "        '''\n",
    "        self.feedforward(X, lambda x: self.actFun(x, type=self.actFun_type))\n",
    "        return np.argmax(self.probs, axis=1)\n",
    "\n",
    "    def backprop(self, X, y):\n",
    "        '''\n",
    "        backprop implements backpropagation to compute the gradients used to update the parameters in the backward step\n",
    "        :param X: input data\n",
    "        :param y: given labels\n",
    "        :return: dL/dW1, dL/b1, dL/dW2, dL/db2\n",
    "        '''\n",
    "\n",
    "        # IMPLEMENT YOUR BACKPROP HERE\n",
    "        num_examples = len(X)\n",
    "        delta3 = self.probs\n",
    "        delta3[range(num_examples), y] -= 1\n",
    "        # dW2 = dL/dW2\n",
    "        # db2 = dL/db2\n",
    "        # dW1 = dL/dW1\n",
    "        # db1 = dL/db1\n",
    "        return dW1, dW2, db1, db2\n",
    "\n",
    "    def fit_model(self, X, y, epsilon=0.01, num_passes=20000, print_loss=True):\n",
    "        '''\n",
    "        fit_model uses backpropagation to train the network\n",
    "        :param X: input data\n",
    "        :param y: given labels\n",
    "        :param num_passes: the number of times that the algorithm runs through the whole dataset\n",
    "        :param print_loss: print the loss or not\n",
    "        :return:\n",
    "        '''\n",
    "        # Gradient descent.\n",
    "        for i in range(0, num_passes):\n",
    "            # Forward propagation\n",
    "            self.feedforward(X, lambda x: self.actFun(x, type=self.actFun_type))\n",
    "            # Backpropagation\n",
    "            dW1, dW2, db1, db2 = self.backprop(X, y)\n",
    "\n",
    "            # Add regularization terms (b1 and b2 don't have regularization terms)\n",
    "            dW2 += self.reg_lambda * self.W2\n",
    "            dW1 += self.reg_lambda * self.W1\n",
    "\n",
    "            # Gradient descent parameter update\n",
    "            self.W1 += -epsilon * dW1\n",
    "            self.b1 += -epsilon * db1\n",
    "            self.W2 += -epsilon * dW2\n",
    "            self.b2 += -epsilon * db2\n",
    "\n",
    "            # Optionally print the loss.\n",
    "            # This is expensive because it uses the whole dataset, so we don't want to do it too often.\n",
    "            if print_loss and i % 1000 == 0:\n",
    "                print(\"Loss after iteration %i: %f\" % (i, self.calculate_loss(X, y)))\n",
    "\n",
    "    def visualize_decision_boundary(self, X, y):\n",
    "        '''\n",
    "        visualize_decision_boundary plots the decision boundary created by the trained network\n",
    "        :param X: input data\n",
    "        :param y: given labels\n",
    "        :return:\n",
    "        '''\n",
    "        plot_decision_boundary(lambda x: self.predict(x), X, y)\n",
    "\n",
    "def main():\n",
    "    # # generate and visualize Make-Moons dataset\n",
    "    # X, y = generate_data()\n",
    "    # plt.scatter(X[:, 0], X[:, 1], s=40, c=y, cmap=plt.cm.Spectral)\n",
    "    # plt.show()\n",
    "\n",
    "    # model = NeuralNetwork(nn_input_dim=2, nn_hidden_dim=3 , nn_output_dim=2, actFun_type='tanh')\n",
    "    # model.fit_model(X,y)\n",
    "    # model.visualize_decision_boundary(X,y)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
